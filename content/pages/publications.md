Title: Publications
Date: 2020-11-13 00:35
Category: Author
Tags: Data Science, Computer Vision
Slug: publications
Authors: Peter Mortimer
Summary: An list of my research publications.

<div class="row">
<div class="col-7 col-sm-7">
<h1 style="text-align:center">Peter Mortimer</h1>
<p><br/><br/></p>
<p style="margin: 0px;">I'm a PhD candidate at the Bundeswehr University in Munich. I'm interested in Machine Learning, Computer Vision, and Data Visualization.</p>
<p><br/></p>
<p style="margin: 0px; text-align: center;"><a href="mailto:peter.mortimer@unibw.de">Email</a> &middot; <a href="https://github.com/tonyromarock">GitHub</a> &middot; <a href="https://scholar.google.com/citations?user=ckEIQp0AAAAJ">Google Scholar</a> &middot; <a href="https://arxiv.org/search/cs?query=Mortimer%2C+Peter&searchtype=author&abstracts=show&order=-announced_date_first">arXiv</a></p>
</div>
<div class="col-5 col-sm-5">
<a href="/images/about/peter_kyoto.jpg"><img style="width:100%;max-width:100%;border-radius: 50%; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3);" src="/images/about/peter_kyoto.jpg"/></a>
</div>
</div>

# Research

Here are all my publcations listed with links to the papers and repositories.

<hr>

<div class="row">
<div class="col-2 col-sm-2">
<a href="/images/publications/icra2025/goose-ex-title-card.jpg" 
data-lightbox="uoa-lightbox" 
data-title="We present an extension of the original GOOSE dataset with recordings from an autonomous excavator and a quadruped robot. Apart from releasing the data, we also showed the advantage of combining the GOOSE and GOOSE-Ex datasets to improve the 2D and 3D segmentation performance on the new robotic platforms." 
data-alt=""><img style="border-radius: 5%; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3);" src="/images/publications/icra2025/goose-ex-title-card.jpg"/></a>
</div>
<div class="col-10 col-sm-10">
<p style="margin: 0px;">Raphael Hagmanns, <b>Peter Mortimer</b>, Miguel Granero, Thorsten Luettel and Janko Petereit</p> 
<p style="margin: 0px;">"Excavating in the Wild: The GOOSE-Ex Dataset for Semantic Segmentation"</p>
<p style="margin: 0px;"><i>International Conference on Robotics and Automation (<b>ICRA</b>)</i>, Atlanta, USA, May 2025.</p>
<p><a href="https://goose-dataset.de/">Project Page</a> &middot; <a href="https://arxiv.org/abs/2409.18788">Paper</a> &middot; <a href="bibtex/icra2025.bib">Bibtex</a></p>  
</div>
</div>

<hr>

<div class="row">
<div class="col-2 col-sm-2">
<a href="/images/publications/wfr-icra2024/outdoor-datasets-survey-title-card.jpg" 
data-lightbox="uoa-lightbox" 
data-title="In this survey, we quantitatively compare publicly available datasets available in unstructured outdoor environments. We focus on datasets for common perception tasks in field robotics. Our survey categorizes and compares available research datasets. This survey also reports on relevant dataset characteristics to help practitioners determine which dataset fits best for their own application." 
data-alt=""><img style="border-radius: 5%; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3);" src="/images/publications/wfr-icra2024/outdoor-datasets-survey-title-card.jpg"/></a>
</div>
<div class="col-10 col-sm-10">
<p style="margin: 0px;"><b>Peter Mortimer</b> and Mirko Maehlisch</p> 
<p style="margin: 0px;">"Survey on Datasets for Perception in Unstructured Outdoor Environments"</p>
<p style="margin: 0px;"><i>Workshop on Field Robotics (<b>W-FR @ ICRA</b>)</i>, Yokohama, Japan, May 2024.</p>
<p><a href="https://arxiv.org/pdf/2404.18750">Paper</a> &middot; <a href="bibtex/wfr-icra2024.bib">Bibtex</a> &middot; <a href="https://norlab-ulaval.github.io/workshop_field_robotics_icra2024/">Workshop Page</a></p>
</div>
</div>

<hr>

<div class="row">
<div class="col-2 col-sm-2">
<a href="/images/publications/icra2024/goose-title-card.jpg" 
data-lightbox="uoa-lightbox" 
data-title="We present the German Outdoor and Offroad Dataset (GOOSE), a comprehensive dataset specifically designed for unstructured outdoor environments. The GOOSE dataset incorporates 10 000 labeled pairs of images and point clouds, which are utilized to train a range of state-of-the-art segmentation models on both image and point cloud data. We open source the dataset, along with an ontology for unstructured terrain, as well as dataset standards and guidelines." 
data-alt=""><img style="border-radius: 5%; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3);" src="/images/publications/icra2024/goose-title-card.jpg"/></a>
</div>
<div class="col-10 col-sm-10">
<p style="margin: 0px;"><b>Peter Mortimer</b>, Raphael Hagmanns, Miguel Granero, Thorsten Luettel, Janko Petereit and Hans-Joachim Wuensche</p> 
<p style="margin: 0px;">"The GOOSE Dataset for Perception in Unstructured Environments"</p>
<p style="margin: 0px;"><i>International Conference on Robotics and Automation (<b>ICRA</b>)</i>, Yokohama, Japan, May 2024.</p>
<p><a href="https://goose-dataset.de/">Project Page</a> &middot; <a href="https://arxiv.org/pdf/2310.16788">Paper</a> &middot; <a href="bibtex/icra2024.bib">Bibtex</a></p>
</div>
</div>

<hr>

<div class="row">
<div class="col-2 col-sm-2">
<a href="/images/publications/iclr2023/Dpt_MonoDepth_Position_and_Scale.png" 
data-lightbox="uoa-lightbox" 
data-title="In this blog post, we investigate the performance of the vision transformer DPT for monocular depth estimation from single images. Here we compare an object's detected depth for DPT and the fully-convolutional MonoDepth when only changing the apparent size or the vertical position of an object." 
data-alt=""><img style="border-radius: 5%; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3);" src="/images/publications/iclr2023/Dpt_MonoDepth_Position_and_Scale.png" /></a>
</div>
<div class="col-10 col-sm-10">
<p style="margin: 0px;"><b>Peter Mortimer</b> and Hans-Joachim Wuensche</p> 
<p style="margin: 0px;">"How Do Vision Transformers See Depth in Single Images?"</p>
<p style="margin: 0px;"><i>Workshop on Scene Representations for Autonomous Driving (<b>SR4AD @ ICLR</b>)</i>, Kigali, Ruwanda, May 2023.</p>
<p><a href="https://sr4ad-vit-mde.github.io/blog/2023/visual-cues-monocular-depth-estimation/">Blog Post</a> &middot; <a href="https://sr4ad-vit-mde.github.io/web-slides/sr4ad/presentation.html">Presentation Slides</a> &middot; <a href="https://youtu.be/0ccu72jOh_k?si=jqaIerl58flkffah&t=2540">Videos</a> &middot; <a href="bibtex/sr4ad-iclr2023.bib">Bibtex</a></p>
</div>
</div>

<hr>

<div class="row">
<div class="col-2 col-sm-2">
<a href="/images/publications/iros2022/title-card.png" 
data-lightbox="uoa-lightbox" 
data-title="TAS-NIR is a novel dataset consisting of 209 semantically segmented and aligned VIS+NIR images in different driving scenarios in unstructured outdoor environments. The fine-grained semantic segmentation of the different vegetation and ground surface types allows closer analysis of VIS+NIR based features. The visible light color image (VIS) and near infrared image (NIR) can be combined to generate vegetation indices like the NDVI image (bottom)." 
data-alt=""><img style="border-radius: 5%; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3);" src="/images/publications/iros2022/title-card.png" /></a>
</div>
<div class="col-10 col-sm-10">
<p style="margin: 0px;"><b>Peter Mortimer</b> and Hans-Joachim Wuensche</p> 
<p style="margin: 0px;">"TAS-NIR: A VIS+NIR Dataset for Fine-grained Semantic Segmentation in Unstructured Outdoor Environments"</p>
<p style="margin: 0px;"><i>Workshop on Planning, Perception and Navigation for Intelligent Vehicles (<b>PPNIV @ IROS</b>)</i>, Kyoto, Japan, October 2022.</p>
<p><a href="https://mucar3.de/iros2022-ppniv-tas-nir">Project Page</a> &middot; <a href="https://project.inria.fr/ppniv22/files/2022/10/PPNIV_TAS-NIR_Paper.pdf">Paper</a> &middot; <a href="https://drive.google.com/uc?export=download&id=1nSyiQDfSvWPMjx4-6Mz01qSda_kPYpsG">Dataset</a> &middot; <a href="bibtex/ppniv-iros2022.bib">Bibtex</a></p>
</div>
</div>

<hr>

<div class="row">
<div class="col-2 col-sm-2">
<a href="/images/publications/icpr2020/image_cube.png" 
data-lightbox="uoa-lightbox" 
data-title="TAS500 is a novel semantic segmentation dataset for autonomous driving in unstructured environments. TAS500 offers fine-grained vegetation and terrain classes to learn drivable surfaces and natural obstacles in outdoor scenes effectively." 
data-alt=""><img style="border-radius: 5%; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3);" src="/images/publications/icpr2020/image_cube.png" /></a>
</div>
<div class="col-10 col-sm-10">
<p style="margin: 0px;">Kai Andreas Metzger, <b>Peter Mortimer</b> and Hans-Joachim Wuensche</p> 
<p style="margin: 0px;">"A Fine-Grained Dataset and its Efficient Semantic Segmentation for Unstructured Driving Scenarios"</p>
<p style="margin: 0px;"><i>International Conference on Pattern Recognition (<b>ICPR</b>)</i>, Milan, Italy, January 2021.</p>
<p><a href="https://mucar3.de/icpr2020-tas500/">Project Page</a> &middot; <a href="https://drive.google.com/file/d/1TeJK-3EBkXzD9FCq4LOLGFRD3lmKqVdp/view">Paper</a> &middot; <a href="https://rzunibw-my.sharepoint.com/:f:/g/personal/thorsten_luettel_rzunibw_onmicrosoft_com/Evdia5SSRaRPuSO2CNsNHY8B3xdK8eHpG-9DSaeEbIJyUw?e=M6NMqI">Dataset</a> &middot; <a href="bibtex/icpr2020.bib">Bibtex</a></p>
</div>
</div>

<hr>

# Talks

Here are all my talks that are publicly available.

<hr>

<div class="row">
<div class="col-10 col-sm-10">
<p style="margin: 0px;">"Einsatz von camera_aravis2 zur Kamerakonfiguration auf Robotersystemen"</p>
<p style="margin: 0px;">ROSCon DE 2024, Karlsruhe, Germany.</p>
<p><a href="https://vimeo.com/1072432639">Video</a> &middot; <a href="https://roscon2024.de/presentations/S2_P4__Einsatz_von_camera_aravis2_zur_Kamerakonfiguration_auf_Robotersystemen.pdf">Slides</a> &middot; <a href="https://github.com/FraunhoferIOSB/camera_aravis2">GitHub</a></p>
</div>
<div class="col-2 col-sm-2">
<a href="/images/publications/rosconde2024/title-card.jpg" 
data-lightbox="uoa-lightbox" 
data-title="Industrial camera drivers like aravis give you control over many components of the image acquisiton process. Raphael Hagmanns (right) and I presented the newest features of camera_aravis2 to the German ROS developer community at the ROSCon DE 2024 in Karlsruhe." 
data-alt=""><img style="border-radius: 5%; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3);" src="/images/publications/rosconde2024/title-card.jpg"/></a>
</div>
</div>

<hr>

<div class="row">
<div class="col-10 col-sm-10">
<p style="margin: 0px;">"20/20 Robot Vision - How to setup cameras in ROS 1 & ROS 2 using camera_aravis"</p>
<p style="margin: 0px;">ROSCon 2022, Kyoto, Japan.</p>
<p><a href="https://vimeo.com/showcase/9954564/video/767140329">Video</a> &middot; <a href="/images/publications/roscon2022/ROSCon2022_PeterMortimer_20-20_Robot_Vision.pdf">Slides</a> &middot; <a href="https://github.com/FraunhoferIOSB/camera_aravis">GitHub</a></p>
</div>
<div class="col-2 col-sm-2">
<a href="/images/publications/roscon2022/title-card.png" 
data-lightbox="uoa-lightbox" 
data-title="Industrial camera drivers like aravis give you control over many components of the image acquisiton process. Here you can observe the difference between minizing the gain (top) and minimizing the exposure time (bottom) on the amount of noise in the image." 
data-alt=""><img style="border-radius: 5%; box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3);" src="/images/publications/roscon2022/title-card.png"/></a>
</div>
</div>


